
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="./theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="./theme/pygments/github.min.css">



  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/solid.css">


  <link rel="shortcut icon" href="./images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="./images/favicon.ico" type="image/x-icon">










 

<meta name="author" content="Walter Fan" />
<meta name="description" content="Daily minute" />
<meta name="keywords" content="journal, blog">


  <meta property="og:site_name" content="Walter Fan's Blog"/>
  <meta property="og:title" content="Deep Stream Inference"/>
  <meta property="og:description" content="Daily minute"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="./deep-stream-inference.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2024-08-04 10:20:00+08:00"/>
  <meta property="article:modified_time" content="2024-08-04 19:30:00+08:00"/>
  <meta property="article:author" content="./author/walter-fan.html">
  <meta property="article:section" content="Journal"/>
  <meta property="article:tag" content="journal"/>
  <meta property="article:tag" content="blog"/>
  <meta property="og:image" content="./images/walterfan.jpg">

  <title>Walter Fan's Blog &ndash; Deep Stream Inference</title>


</head>
<body class="light-theme">

<aside>
  <div>
    <a href="./">
      <img src="./images/walterfan.jpg" alt="Walter Fan" title="Walter Fan">
    </a>

    <h1>
      <a href="./">Walter Fan</a>
    </h1>

    <p>手握灵珠常奋笔, 心开天籁不吹箫</p>


    <nav>
      <ul class="list">



          <li>
            <a target="_self" href="/tao" >tao</a>
          </li>
          <li>
            <a target="_self" href="interest.html" >interest</a>
          </li>
          <li>
            <a target="_self" href="/wordpress" >notebook</a>
          </li>
          <li>
            <a target="_self" href="bookmark.html" >bookmark</a>
          </li>
          <li>
            <a target="_self" href="/webrtc/examples/index.html" >webrtc</a>
          </li>
          <li>
            <a target="_self" href="https://github.com/walterfan" >github</a>
          </li>
          <li>
            <a target="_self" href="https://www.jianshu.com/u/e0b365801f48" >技术文章</a>
          </li>
          <li>
            <a target="_self" href="/tao/tech" >技术笔记</a>
          </li>
          <li>
            <a target="_self" href="/tao/tool" >我的工具</a>
          </li>
          <li>
            <a target="_self" href="/tao" >给我留言</a>
          </li>
      </ul>
    </nav>

    <ul class="social">
      <li>
        <a class="sc-github"
           href="http://github.com/walterfan"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
      <li>
        <a class="sc-weibo"
           href="http://weibo.com/walterfan"
           target="_blank">
          <i class="fa-brands fa-weibo"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="deep-stream-inference">Deep Stream Inference</h1>
    <p>
      Posted on Sun 04 August 2024 in <a href="./category/journal.html">Journal</a>

    </p>
  </header>


  <div>
    <table>
<thead>
<tr>
<th><strong>Abstract</strong></th>
<th>Deep Stream Inference</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Authors</strong></td>
<td><a href="https://www.fanyamin.com" referrerpolicy="no-referrer" rel="noopener noreferrer" target="_blank" title="Walter Fan">Walter Fan</a></td>
</tr>
<tr>
<td> <strong>Category</strong>  </td>
<td> learning note  </td>
</tr>
<tr>
<td><strong>Status</strong></td>
<td>v1.0</td>
</tr>
<tr>
<td><strong>Updated</strong></td>
<td>2024-08-04</td>
</tr>
<tr>
<td><strong>License</strong></td>
<td><a href="http://creativecommons.org/licenses/by-nc-nd/4.0" referrerpolicy="no-referrer" rel="noopener noreferrer" target="_blank" title="CC-BY-NC-ND 4.0">CC-BY-NC-ND 4.0</a></td>
</tr>
</tbody>
</table>
<p>Let's take a look for the example -- it uses DeepStream elements for a single H.264 stream:
filesrc → decode → nvstreammux → nvinfer (primary detector) → nvdsosd → renderer.</p>
<p>This app uses resnet10.caffemodel for detection.</p>
<div class="highlight"><pre><span></span><code>  ds_example_1:
    desc: deep stream example 1
    tags: jetson
    steps:
      # 读取 H264 编码的视频文件
      - filesrc location=/opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.h264
      - h264parse # 解析 h264 文件流为 video/x-h264 字节流
      - nvv4l2decoder # 将 video/x-h264 字节流为 NV12 格式
      - nvstreammux0.sink_0 nvstreammux batch-size=1 width=1920 height=1080 batched-push-timeout=40000
      - nvinfer config-file-path=/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-test1/dstest1_pgie_config.txt
      - nvvideoconvert # 转换视频格式
      - nvdsosd       # 画框
      - autovideosink # 显示视频
</code></pre></div>

<p>The core element is nvinfer that using TensorRT for inference</p>
<p>效果如下</p>
<p><img alt="ds-example-1" src="./images/ds-example-1.png"></p>
<h2 id="gst-nvinfer">Gst-nvinfer</h2>
<p>The Gst-nvinfer plugin does inferencing on input data using NVIDIA® TensorRT™.</p>
<p>The plugin accepts batched NV12/RGBA buffers from upstream. The NvDsBatchMeta structure must already be attached to the Gst Buffers.</p>
<p>The low-level library (libnvds_infer) operates on any of INT8 RGB, BGR, or GRAY data with dimension of Network Height and Network Width.</p>
<p>The Gst-nvinfer plugin performs transforms (format conversion and scaling), on the input frame based on network requirements, and passes the transformed data to the low-level library. The low-level library preprocesses the transformed frames (performs normalization and mean subtraction) and produces final float RGB/BGR/GRAY planar data which is passed to the TensorRT engine for inferencing. The output type generated by the low-level library depends on the network type. The pre-processing function is:</p>
<div class="highlight"><pre><span></span><code>y = net scale factor*(x-mean)
</code></pre></div>

<p>Where:</p>
<ul>
<li>
<p>x is the input pixel value. It is an int8 with range [0,255].</p>
</li>
<li>
<p>mean is the corresponding mean value, read either from the mean file or as offsets[c], where c is the channel to which the input pixel belongs, and offsets is the array specified in the configuration file. It is a float.</p>
</li>
<li>
<p>net-scale-factor is the pixel scaling factor specified in the configuration file. It is a float.</p>
</li>
<li>
<p>y is the corresponding output pixel value. It is a float.</p>
</li>
</ul>
<p>Gst-nvinfer currently works on the following type of networks:</p>
<ul>
<li>
<p>Multi-class object detection 多类别对象检测</p>
</li>
<li>
<p>Multi-label classification 多标签分类</p>
</li>
<li>
<p>Segmentation (semantic) 语义分割</p>
</li>
<li>
<p>Instance Segmentation 实例分割</p>
</li>
</ul>
<p>The Gst-nvinfer plugin can work in three modes:</p>
<ul>
<li>
<p>Primary mode: Operates on full frames - 主要模式：对全帧进行操作</p>
</li>
<li>
<p>Secondary mode: Operates on objects added in the meta by upstream components - 次要模式：对上游组件在元中添加的对象进行操作</p>
</li>
<li>
<p>Preprocessed Tensor Input mode: Operates on tensors attached by upstream components - 预处理张量输入模式：对上游组件附加的张量进行操作</p>
</li>
</ul>
<p>在预处理张量输入模式下运行时，Gst-nvinfer 内部的预处理被完全跳过。插件会查找附加到输入缓冲区的 GstNvDsPreProcessBatchMeta，并将张量按原样传递给 TensorRT 推理函数，而无需进行任何修改。此模式目前支持全帧和 ROI 处理。GstNvDsPreProcessBatchMeta 由 Gst-nvdspreprocess 插件附加。</p>
<p>当插件与跟踪器一起作为辅助分类器运行时，它会尝试通过避免在每一帧中对相同的对象进行重新推理来提高性能。它通过将分类输出缓存在以对象的唯一 ID 为键的映射中来实现这一点。仅当对象在帧中首次出现（基于其对象 ID）或对象的大小（边界框面积）增加 20% 或更多时，才会推断该对象。只有将跟踪器添加为上游元素时，才可以进行此优化。</p>
<p><img alt="nvinver" src="./images/nvinfer.png"></p>
<h2 id="notes">notes</h2>
<ul>
<li>TNR - Temporal Noise Reduction</li>
<li>VPI - Vision Programming Interface</li>
<li>PVA - Programmable Vision Accelerator</li>
<li>VIC - Video and Image Compositor</li>
<li>OFA - Optical Flow Accelerator</li>
</ul>
<h3 id="nv12">NV12</h3>
<p>NV12是一种广泛使用的视频编解码颜色编码格式，它采用YUV 4:2:0的采样方式，意味着每四个Y分量共享一组UV分量。在NV12格式中，亮度信息（Y分量）和色度信息（UV分量）是分开存储的，其中Y分量占据一个平面，UV分量交替存储于另一个平面，具体的存储顺序是YYYYY...后跟着UVUV...，即U和V分量是交错存储的 。这种格式在视频传输和图像处理中非常常见，因为它在保持图像质量的同时，可以有效地减少数据量，降低存储和传输的带宽需求 。</p>
<p>NV12格式的一个优点是它在GPU中的处理效率很高，因为它只有两个平面，并且UV平面的宽度（pitch）和亮度平面相同，高度是亮度平面的一半，这使得在很多场景下可以高效地一起处理，而不需要分为两个或三个平面分别操作 。此外，NV12格式也是iOS相机（AVCaptureOutput）可直接输出的视频帧格式之一，它在iOS上分为Full Range和Video Range两种，区别在于亮度（Y）分量的取值范围 。</p>
<p>在实际应用中，NV12格式的图像数据可以通过特定的函数进行高效的拷贝，例如Windows SDK提供的MFCopyImage函数，它可以提高拷贝效率，尤其是在视频解码或实时直播场景中 。此外，NV12格式的图像数据还可以转换为其他格式，如RGB，这在图像后期处理和显示中非常有用 。</p>
<p>总的来说，NV12是一种高效的YUV颜色编码格式，适用于多种图像和视频处理应用，它通过减少色度分量的采样率，在保证图像质量的同时降低了数据量，提高了处理效率</p>
<h3 id="gst-nvstreammux">Gst-nvstreammux</h3>
<p>The Gst-nvstreammux plugin forms a batch of frames from multiple input sources.</p>
<p>When connecting a source to nvstreammux (the muxer), a new pad must be requested from the muxer using gst_element_get_request_pad() and the pad template sink_%u.</p>
<p>For more information, see link_element_to_streammux_sink_pad() in the DeepStream app source code.</p>
<p>The muxer forms a batched buffer of batch-size frames. (batch-size is specified using the gst object property.)</p>
<p>If the muxer’s output format and input format are the same, the muxer forwards the frames from that source as a part of the muxer’s output batched buffer.</p>
<p>The frames are returned to the source when muxer gets back its output buffer.</p>
<p>If the resolution is not the same, the muxer scales frames from the input into the batched buffer and then returns the input buffers to the upstream component.</p>
<p>The muxer pushes the batch downstream when the batch is filled, or the batch formation timeout batched-pushed-timeout is reached.</p>
<p>The timeout starts running when the first buffer for a new batch is collected.</p>
<p>The muxer uses a round-robin algorithm to collect frames from the sources.</p>
<p>It tries to collect an average of (batch-size/num-source) frames per batch from each source (if all sources are live and their frame rates are all the same).</p>
<p>The number varies for each source, though, depending on the sources’ frame rates.</p>
<p>The muxer outputs a single resolution (i.e. all frames in the batch have the same resolution).</p>
<p>This resolution can be specified using the width and height properties.</p>
<p>The muxer scales all input frames to this resolution.</p>
<p>The enable-padding property can be set to true to preserve the input aspect ratio while scaling by padding with black bands.</p>
<p>For DGPU platforms, the GPU to use for scaling and memory allocations can be specified with the gpu-id property.</p>
<p>For each source that needs scaling to the muxer’s output resolution, the muxer creates a buffer pool and allocates four buffers each of size:</p>
<div class="highlight"><pre><span></span><code>output width*output height*f
</code></pre></div>

<p>Where f is 1.5 for NV12 format, or 4.0 for RGBA.</p>
<p>The memory type is determined by the nvbuf-memory-type property. Set the live-source property to true to inform the muxer that the sources are live.</p>
<p>In this case the muxer attaches the PTS of the last copied input buffer to the batched Gst Buffer’s PTS.</p>
<p>If the property is set to false, the muxer calculates timestamps based on the frame rate of the source which first negotiates capabilities with the muxer.</p>
<p>The muxer attaches an NvDsBatchMeta metadata structure to the output batched buffer. This meta contains information about the frames copied into the batch (e.g. source ID of the frame, original resolutions of the input frames, original buffer PTS of the input frames).</p>
<p>The source connected to the Sink_N pad will have pad_index N in NvDsBatchMeta.</p>
<p>The muxer supports addition and deletion of sources at run time.</p>
<p>When the muxer receives a buffer from a new source, it sends a GST_NVEVENT_PAD_ADDED event.</p>
<p>When a muxer sink pad is removed, the muxer sends a GST_NVEVENT_PAD_DELETED event. Both events contain the source ID of the source being added or removed (see sources/includes/gst-nvevent.h).</p>
<p>Downstream elements can reconfigure when they receive these events. Additionally, the muxer also sends a GST_NVEVENT_STREAM_EOS to indicate EOS from the source. The muxer supports calculation of NTP timestamps for source frames. It supports two modes. In the system timestamp mode, the muxer attaches the current system time as NTP timestamp. In the RTCP timestamp mode, the muxer uses RTCP Sender Report to calculate NTP timestamp of the frame when the frame was generated at source. The NTP timestamp is set in ntp_timestamp field of NvDsFrameMeta. The mode can be toggled by setting the attach-sys-ts property. For more details, refer to section NTP Timestamp in DeepStream.</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://docs.nvidia.com/metropolis/deepstream/6.0/dev-guide/index.html" referrerpolicy="no-referrer" rel="noopener noreferrer" target="_blank" title="DeepStream 6 Developer Guide">DeepStream 6 Developer Guide</a></li>
<li><a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvvideo4linux2.html" referrerpolicy="no-referrer" rel="noopener noreferrer" target="_blank" title="Gst-nvvideo4linux2 plugin">Gst-nvvideo4linux2 plugin</a></li>
<li><a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvstreammux.html" referrerpolicy="no-referrer" rel="noopener noreferrer" target="_blank" title="Gst-nvstreammux plugin">Gst-nvstreammux plugin</a></li>
<li><a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvinfer.html" referrerpolicy="no-referrer" rel="noopener noreferrer" target="_blank" title="Gst-nvinfer plugin">Gst-nvinfer plugin</a></li>
<li><a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html" referrerpolicy="no-referrer" rel="noopener noreferrer" target="_blank" title="TensorRT developer guide">TensorRT developer guide</a></li>
</ul>
<hr/>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-nd/4.0/" referrerpolicy="no-referrer" rel="noopener noreferrer" target="_blank" title="知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</a>进行许可。</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="./tag/journal.html">journal</a>
      <a href="./tag/blog.html">blog</a>
    </p>
  </div>


  <div class="neighbors">
    <a class="btn float-left" href="./du-li-kai-fa-cong-na-li-kai-shi.html" title="独立开发从哪里开始">
      <i class="fa fa-angle-left"></i> Previous Post
    </a>
    <a class="btn float-right" href="./mi-ma-dao-di-zen-yao-cun-fang-cai-an-quan.html" title="密码到底怎么存放才安全?">
      Next Post <i class="fa fa-angle-right"></i>
    </a>
  </div>

  <div class="related-posts">
    <h4>You might enjoy</h4>
    <ul class="related-posts">
      <li><a href="./nacos-and-its-c-sdk.html">Nacos and its C++ SDK</a></li>
      <li><a href="./pao-zai-shui-li-cai-neng-xue-hui-you-yong-qian-duan-kai-fa-xue-xi-zhi-lu.html">泡在水里才能学会游泳 - 前端开发学习之路</a></li>
      <li><a href="./tong-guo-tong-xin-lai-gong-xiang-nei-cun-er-bu-shi-tong-guo-gong-xiang-nei-cun-lai-tong-xin.html">通过通信来共享内存, 而不是通过共享内存来通信</a></li>
      <li><a href="./go-yu-yan-de-chang-jian-xian-jing.html">go 语言的常见陷阱</a></li>
      <li><a href="./bian-che-mo-shi-de-xie-yi-she-ji.html">边车模式的协议设计</a></li>
    </ul>
  </div>



<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'wfblog';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
    Please enable JavaScript to view comments.
</noscript>
<!-- End Disqus -->
</article>

<footer>
<p>&copy; 2010 ~ 2030  Walter Fan <a href="https://beian.miit.gov.cn" target="_blank">皖ICP备20001876号-1</a></p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p><!-- StatusCake -->

<!-- End StatusCake --></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Walter Fan's Blog ",
  "url" : ".",
  "image": "./images/walterfan.jpg",
  "description": "an old programmer never die, he just branch to a new address."
}
</script>
</body>
</html>