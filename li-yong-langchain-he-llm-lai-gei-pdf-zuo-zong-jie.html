
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="./theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="./theme/pygments/github.min.css">



  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/solid.css">


  <link rel="shortcut icon" href="./images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="./images/favicon.ico" type="image/x-icon">










 

<meta name="author" content="Walter Fan" />
<meta name="description" content="Daily minute" />
<meta name="keywords" content="journal, blog">


  <meta property="og:site_name" content="Walter Fan's Blog"/>
  <meta property="og:title" content="利用 langchain 和 LLM 来给 PDF 做总结"/>
  <meta property="og:description" content="Daily minute"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="./li-yong-langchain-he-llm-lai-gei-pdf-zuo-zong-jie.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2024-10-01 10:20:00+08:00"/>
  <meta property="article:modified_time" content="2024-10-01 19:30:00+08:00"/>
  <meta property="article:author" content="./author/walter-fan.html">
  <meta property="article:section" content="Journal"/>
  <meta property="article:tag" content="journal"/>
  <meta property="article:tag" content="blog"/>
  <meta property="og:image" content="./images/walterfan.jpg">

  <title>Walter Fan's Blog &ndash; 利用 langchain 和 LLM 来给 PDF 做总结</title>


</head>
<body class="light-theme">

<aside>
  <div>
    <a href="./">
      <img src="./images/walterfan.jpg" alt="Walter Fan" title="Walter Fan">
    </a>

    <h1>
      <a href="./">Walter Fan</a>
    </h1>

    <p>手握灵珠常奋笔, 心开天籁不吹箫</p>


    <nav>
      <ul class="list">



          <li>
            <a target="_self" href="tao.html" >tao</a>
          </li>
          <li>
            <a target="_self" href="interest.html" >interest</a>
          </li>
          <li>
            <a target="_self" href="/wordpress" >notebook</a>
          </li>
          <li>
            <a target="_self" href="bookmark.html" >bookmark</a>
          </li>
          <li>
            <a target="_self" href="/webrtc/examples/index.html" >webrtc</a>
          </li>
          <li>
            <a target="_self" href="https://github.com/walterfan" >github</a>
          </li>
          <li>
            <a target="_self" href="https://www.jianshu.com/u/e0b365801f48" >技术文章</a>
          </li>
      </ul>
    </nav>

    <ul class="social">
      <li>
        <a class="sc-github"
           href="http://github.com/walterfan"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="li-yong-langchain-he-llm-lai-gei-pdf-zuo-zong-jie">利用 langchain 和 LLM 来给 PDF 做总结</h1>
    <p>
      Posted on Tue 01 October 2024 in <a href="./category/journal.html">Journal</a>

    </p>
  </header>


  <div>
    <table>
<thead>
<tr>
<th><strong>Abstract</strong></th>
<th>利用 langchain 和 LLM 来给 PDF 做总结</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Authors</strong></td>
<td><a href="https://www.fanyamin.com">Walter Fan</a></td>
</tr>
<tr>
<td> <strong>Category</strong>  </td>
<td> learning note  </td>
</tr>
<tr>
<td><strong>Status</strong></td>
<td>v1.0</td>
</tr>
<tr>
<td><strong>Updated</strong></td>
<td>2024-10-01</td>
</tr>
<tr>
<td><strong>License</strong></td>
<td><a href="http://creativecommons.org/licenses/by-nc-nd/4.0">CC-BY-NC-ND 4.0</a></td>
</tr>
</tbody>
</table>
<p>在网上看到一个PDF, 讲的是 Gstreamer 的的动态管道的构建, 一瞥而过, 没时间细看, 先写个小程序通过 langchain 和 LLM 给它做个快速总结</p>
<h2 id="_1">代码如下</h2>
<div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.document_loaders</span><span class="w"> </span><span class="kn">import</span> <span class="n">UnstructuredPDFLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.llms</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLMChain</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="c1"># 加载 PDF 文件</span>
<span class="n">pdf_loader</span> <span class="o">=</span> <span class="n">UnstructuredPDFLoader</span><span class="p">(</span><span class="s2">&quot;path_to_your_pdf_file.pdf&quot;</span><span class="p">)</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">pdf_loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># 获取 PDF 的纯文本内容</span>
<span class="n">pdf_text</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">])</span>

<span class="c1"># 创建 LLM 对象 (使用 OpenAI GPT)</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">openai_api_key</span><span class="o">=</span><span class="s2">&quot;your_openai_api_key&quot;</span><span class="p">)</span>

<span class="c1"># 定义总结的 Prompt</span>
<span class="n">prompt_template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">请总结以下内容：</span>
<span class="si">{pdf_text}</span>
<span class="s2">总结：</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;pdf_text&quot;</span><span class="p">],</span>
    <span class="n">template</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 创建 LLMChain</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">)</span>

<span class="c1"># 使用 LLM 生成总结</span>
<span class="n">summary</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">pdf_text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PDF 总结：</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">summary</span><span class="p">)</span>
</pre></div>


<h2 id="_2">代码输出如下</h2>
<div class="highlight"><pre><span></span>PDF summary:
 总结内容：

1. **演讲者信息**：
   - 演讲者：José Antonio Santos Cadenas
   - 职位：软件工程师
   - 教育背景：Telematic Systems 硕士
   - 工作经历：Kurento Media Server (KMS) 管理员
   - 联系方式：santoscadenas@gmail.com

2. **GStreamer 静态管道**：
   - 使用 `gst-launch` 命令创建复杂的媒体管道，例如将视频文件转码。
   - 示例：`gst-launch-1.0 filesrc location=sample.mp4 ! qtdemux ! avdec_h264 ! queue ! vp8enc ! webmmux ! filesink location=sample.webm`

3. **GStreamer 动态元素**：
   - GStreamer 提供了一些动态元素（如 `autovideosrc`, `autovideosink`, `decodebin`, `playbin`），简化了动态管道的创建。
   - 示例：`gst-launch-1.0 filesrc location=sample.mp4 ! decodebin ! vp8enc ! webmmux ! filesink location=sample.webm`

4. **动态管道不依赖于媒体**：
   - 动态管道可以根据媒体流而不是外部条件创建。
   - 例如：根据用户操作添加或替换元素。

5. **创建动态管道的要求**：
   - 需要深入理解 GStreamer 的工作原理，包括媒体如何在 pads 之间流动、协商如何进行、流线程如何工作以及 probes 如何工作。

6. **在播放时添加和移除元素**：
   - 错误方式：直接调用 `gst_pad_unlink`，可能导致死锁或竞争条件。
   - 正确方式：使用 probe 等待 pad 空闲，确保在断开连接时不会影响媒体流。

7. **在 tee 后添加元素**：
   - tee 会处理断开的 pads，但建议在 idle 或 block 回调中处理元素连接，以避免状态更改或协商期间的问题。

8. **注意协商**：
   - 在 tee 后连接元素时，需要注意协商可能会影响其他分支，建议使用 `capsfilter` 或转换器来简化协商。

9. **移除元素**：
   - 有时需要允许元素处理所有队列中的缓冲区（例如录制时），断开连接后需要发送 EOS 并等待管道末端处理。

10. **实时管道建议**：
    - 在实时管道中，元素应配置为尽可能快地工作，并添加队列以分离不同线程中的处理。

11. **总结建议**：
    - 动态管道不容易，理解要尝试的操作及其对管道其他部分的影响。
    - 断开连接前阻塞 pads，连接前同步新元素的状态，始终先连接 sink 元素以避免媒体泄漏。

12. **联系方式和资源**：
    - 演讲者邮箱：santoscadenas@gmail.com
    - Kurento 相关链接：[Kurento](http://www.kurento.org), [GitHub](http://www.github.com/kurento), [Twitter](https://twitter.com/kurentoms)
    - 其他相关组织：[NUBOMEDIA](http://www.nubomedia.eu), [FI-WARE](http://www.fi-ware.org), [EU](http://ec.europa.eu)
</pre></div>


<h2 id="_3">代码解释</h2>
<p>这段代码的作用是从 PDF 文件中提取文本内容，并使用 OpenAI 的 GPT 模型对其进行总结。以下是各部分的解释：</p>
<h3 id="1">1. 导入必要的库</h3>
<div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.document_loaders</span><span class="w"> </span><span class="kn">import</span> <span class="n">UnstructuredPDFLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.llms</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLMChain</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span>
</pre></div>


<ul>
<li><strong><code>UnstructuredPDFLoader</code></strong>: 用于加载和解析 PDF 文件，将其转换为文本格式。</li>
<li><strong><code>OpenAI</code></strong>: 用于与 OpenAI 的 GPT 模型进行交互，生成自然语言的输出。</li>
<li><strong><code>LLMChain</code></strong>: 用于将模型与输入的提示（prompt）结合起来，创建一个完整的流程链。</li>
<li><strong><code>PromptTemplate</code></strong>: 用于定义向 GPT 提供的提示模板。</li>
</ul>
<h3 id="2-pdf">2. 加载 PDF 文件</h3>
<div class="highlight"><pre><span></span><span class="n">pdf_loader</span> <span class="o">=</span> <span class="n">UnstructuredPDFLoader</span><span class="p">(</span><span class="s2">&quot;path_to_your_pdf_file.pdf&quot;</span><span class="p">)</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">pdf_loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
</pre></div>


<ul>
<li><strong><code>UnstructuredPDFLoader</code></strong>: 用来加载 PDF 文件。它会解析 PDF 并将其转换成文档对象列表，每个文档对象包含了对应的页面内容。</li>
<li><strong><code>documents</code></strong>: 加载后的 PDF 文件内容存储在 <code>documents</code> 列表中，每个文档对象对应一页 PDF 文本。</li>
</ul>
<h3 id="3-pdf">3. 提取 PDF 文本</h3>
<div class="highlight"><pre><span></span><span class="n">pdf_text</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">])</span>
</pre></div>


<ul>
<li><strong><code>pdf_text</code></strong>: 从文档对象中提取纯文本内容，将每一页的内容拼接在一起，生成完整的 PDF 文本。</li>
</ul>
<h3 id="4-openai-llm">4. 创建 OpenAI LLM 对象</h3>
<div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">openai_api_key</span><span class="o">=</span><span class="s2">&quot;your_openai_api_key&quot;</span><span class="p">)</span>
</pre></div>


<ul>
<li><strong><code>OpenAI</code></strong>: 实例化 GPT 模型，<code>temperature=0.7</code> 设置了生成文本的随机性，较高的值会导致模型生成更多样化的输出。需要使用 OpenAI 的 API 密钥来访问 GPT 服务。</li>
</ul>
<h3 id="5-prompt">5. 定义 Prompt 模板</h3>
<div class="highlight"><pre><span></span><span class="n">prompt_template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">请总结以下内容：</span>
<span class="si">{pdf_text}</span>
<span class="s2">总结：</span>
<span class="s2">&quot;&quot;&quot;</span>
</pre></div>


<ul>
<li><strong><code>prompt_template</code></strong>: 定义了一个自然语言提示模板，包含一个占位符 <code>{pdf_text}</code>，用于替换成提取的 PDF 文本。该模板告诉 GPT 模型对 PDF 内容进行总结。</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;pdf_text&quot;</span><span class="p">],</span>
    <span class="n">template</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>


<ul>
<li><strong><code>PromptTemplate</code></strong>: 创建一个具体的提示模板，指定 <code>pdf_text</code> 作为输入变量。</li>
</ul>
<h3 id="6-llmchain">6. 创建 LLMChain</h3>
<div class="highlight"><pre><span></span><span class="n">chain</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">)</span>
</pre></div>


<ul>
<li><strong><code>LLMChain</code></strong>: 将 GPT 模型和定义好的提示模板连接起来，形成一个可执行的链，链条会根据提示和模型生成输出。</li>
</ul>
<h3 id="7">7. 生成总结</h3>
<div class="highlight"><pre><span></span><span class="n">summary</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">pdf_text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PDF 总结：</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">summary</span><span class="p">)</span>
</pre></div>


<ul>
<li><strong><code>chain.run(pdf_text)</code></strong>: 执行链，将提取的 PDF 文本传递给 LLM，GPT 模型根据提供的 prompt 对 PDF 文本进行总结。</li>
<li><strong><code>print("PDF 总结：\n", summary)</code></strong>: 打印生成的总结。</li>
</ul>
<h3 id="_4">总结</h3>
<p>该代码从 PDF 文件中提取纯文本内容，使用 OpenAI GPT 模型对其进行总结。你可以通过修改 prompt 或调整模型的参数来生成不同风格的总结。</p>
<p>你可以将此代码应用于任何 PDF 文件，得到简明的总结内容。
<hr/>
本作品采用<a href="http://creativecommons.org/licenses/by-nc-nd/4.0/">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</a>进行许可。</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="./tag/journal.html">journal</a>
      <a href="./tag/blog.html">blog</a>
    </p>
  </div>


  <div class="neighbors">
    <a class="btn float-left" href="./yong-logfire-ti-gao-ying-yong-de-ke-guan-ce-xing.html" title="用 logfire 提高应用的可观测性">
      <i class="fa fa-angle-left"></i> Previous Post
    </a>
    <a class="btn float-right" href="./asyncio-summary.html" title="asyncio summary">
      Next Post <i class="fa fa-angle-right"></i>
    </a>
  </div>

  <div class="related-posts">
    <h4>You might enjoy</h4>
    <ul class="related-posts">
      <li><a href="./cheng-xu-yuan-ba-ba-xie-gei-nu-er-de-qi-feng-xin.html">程序员爸爸写给女儿的七封信</a></li>
      <li><a href="./ru-he-ying-dui-yi-tuan-luan-ma.html">如何应对一团乱码</a></li>
      <li><a href="./wo-he-ni-wo-yu-da-mo-xing.html">我和你 - 我与大模型</a></li>
      <li><a href="./an-quan-xu-yao-ge-chi-ge-chi-cai-neng-an-quan.html">安全需要隔离, 隔离才能安全</a></li>
      <li><a href="./ji-bu-de-na-yao-duo-jiu-ya-suo-yi-xia-ba.html">记不得那么多就压缩一下吧</a></li>
    </ul>
  </div>



<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'wfblog';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
    Please enable JavaScript to view comments.
</noscript>
<!-- End Disqus -->
</article>

<footer>
<p>&copy; 2010 ~ 2030  Walter Fan <a href="https://beian.miit.gov.cn" target="_blank">皖ICP备20001876号-1</a></p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p><!-- StatusCake -->

<!-- End StatusCake --></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Walter Fan's Blog ",
  "url" : ".",
  "image": "./images/walterfan.jpg",
  "description": "an old programmer never die, he just branch to a new address."
}
</script>
</body>
</html>