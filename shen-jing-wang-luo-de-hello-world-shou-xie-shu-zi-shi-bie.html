
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="./theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="./theme/pygments/github.min.css">



  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/solid.css">


  <link rel="shortcut icon" href="./images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="./images/favicon.ico" type="image/x-icon">










 

<meta name="author" content="Walter Fan" />
<meta name="description" content="Daily minute" />
<meta name="keywords" content="journal, blog">


  <meta property="og:site_name" content="Walter Fan's Blog"/>
  <meta property="og:title" content="神经网络的 Hello World - 手写数字识别"/>
  <meta property="og:description" content="Daily minute"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="./shen-jing-wang-luo-de-hello-world-shou-xie-shu-zi-shi-bie.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2024-07-13 10:20:00+08:00"/>
  <meta property="article:modified_time" content="2024-07-13 19:30:00+08:00"/>
  <meta property="article:author" content="./author/walter-fan.html">
  <meta property="article:section" content="Journal"/>
  <meta property="article:tag" content="journal"/>
  <meta property="article:tag" content="blog"/>
  <meta property="og:image" content="./images/walterfan.jpg">

  <title>Walter Fan's Blog &ndash; 神经网络的 Hello World - 手写数字识别</title>


</head>
<body class="light-theme">

<aside>
  <div>
    <a href="./">
      <img src="./images/walterfan.jpg" alt="Walter Fan" title="Walter Fan">
    </a>

    <h1>
      <a href="./">Walter Fan</a>
    </h1>

    <p>手握灵珠常奋笔, 心开天籁不吹箫</p>


    <nav>
      <ul class="list">



          <li>
            <a target="_blank" href="tao.html" >tao</a>
          </li>
          <li>
            <a target="_blank" href="article.html" >article</a>
          </li>
          <li>
            <a target="_blank" href="interest.html" >interest</a>
          </li>
          <li>
            <a target="_self" href="/wordpress" >notebook</a>
          </li>
          <li>
            <a target="_blank" href="bookmark.html" >bookmark</a>
          </li>
          <li>
            <a target="_blank" href="manual.html" >manual</a>
          </li>
          <li>
            <a target="_self" href="/webrtc/examples/index.html" >webrtc</a>
          </li>
          <li>
            <a target="_blank" href="https://github.com/walterfan" >github</a>
          </li>
      </ul>
    </nav>

    <ul class="social">
      <li>
        <a class="sc-github"
           href="http://github.com/walterfan"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="shen-jing-wang-luo-de-hello-world-shou-xie-shu-zi-shi-bie">神经网络的 Hello World - 手写数字识别</h1>
    <p>
      Posted on Sat 13 July 2024 in <a href="./category/journal.html">Journal</a>

    </p>
  </header>


  <div>
    <table>
<thead>
<tr>
<th><strong>Abstract</strong></th>
<th>神经网络的 Hello World - 手写数字识别</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Authors</strong></td>
<td><a href="https://www.fanyamin.com">Walter Fan</a></td>
</tr>
<tr>
<td> <strong>Category</strong>  </td>
<td> learning note  </td>
</tr>
<tr>
<td><strong>Status</strong></td>
<td>v1.0</td>
</tr>
<tr>
<td><strong>Updated</strong></td>
<td>2024-07-13</td>
</tr>
<tr>
<td><strong>License</strong></td>
<td><a href="http://creativecommons.org/licenses/by-nc-nd/4.0">CC-BY-NC-ND 4.0</a></td>
</tr>
</tbody>
</table>
<p>就象编程语言中的 hello world 那样, 手写数字识别是神经网络入门最常用的例子.
它的训练和测试数据集可通过 https://www.kaggle.com/datasets/oddrationale/mnist-in-csv 下载</p>
<p>它分为两部分:</p>
<ol>
<li>mnist_train.csv 训练数据集</li>
<li>mnist_test.csv  测试数据集</li>
</ol>
<p>mnist_train.csv 文件包含 60,000 个训练示例和标签。mnist_test.csv 包含 10,000 个测试示例和标签。每行由 785 个值组成：第一个值是标签（从 0 到 9 的数字），其余 784 个值是像素值（从 0 到 255 的数字）。</p>
<h2 id="_1">神经网络入门</h2>
<p>神经网络（Neural Networks）是一种受人脑结构启发的计算模型，它模仿人脑中神经元的连接和交互方式来处理信息。神经网络由一系列节点（或称为“神经元”）组成，这些节点在层（或称为“层级”）中组织。每个节点可以接收输入，对输入进行加权求和，然后通过一个非线性激活函数来生成输出。</p>
<p>神经网络的一些关键概念有：</p>
<ol>
<li><strong>神经元（Neuron）</strong>：</li>
<li>
<p>神经网络的基本单元，每个神经元接收一组输入值，进行加权求和，并可能添加一个偏置项。</p>
</li>
<li>
<p><strong>层（Layer）</strong>：</p>
</li>
<li>神经元的集合，通常分为输入层、隐藏层和输出层。</li>
<li>输入层接收原始数据。</li>
<li>隐藏层是中间层，可以有多个，用于提取特征和进行非线性变换。</li>
<li>
<p>输出层产生最终的预测或分类结果。</p>
</li>
<li>
<p><strong>权重（Weight）</strong>：</p>
</li>
<li>
<p>每个神经元的输入都有一个权重，这些权重决定了输入信号对神经元输出的影响大小。</p>
</li>
<li>
<p><strong>偏置（Bias）</strong>：</p>
</li>
<li>
<p>加在加权求和的输入上的一个值，用于调整神经元激活的阈值。</p>
</li>
<li>
<p><strong>激活函数（Activation Function）</strong>：</p>
</li>
<li>一个数学函数，用于在神经元的加权求和之后引入非线性，使得网络能够学习和执行更复杂的任务。</li>
<li>
<p>常见的激活函数包括 Sigmoid、Tanh、ReLU（Rectified Linear Unit）等。</p>
</li>
<li>
<p><strong>前向传播（Forward Propagation）</strong>：</p>
</li>
<li>
<p>数据在神经网络中的流动方向，从输入层通过隐藏层到输出层。</p>
</li>
<li>
<p><strong>损失函数（Loss Function）</strong>：</p>
</li>
<li>
<p>衡量模型预测值与实际值差异的函数，用于指导模型训练过程中的优化。</p>
</li>
<li>
<p><strong>反向传播（Backpropagation）</strong>：</p>
</li>
<li>
<p>一种训练神经网络的算法，通过计算损失函数关于网络参数的梯度，并利用这些梯度来更新网络的权重和偏置。</p>
</li>
<li>
<p><strong>优化器（Optimizer）</strong>：</p>
</li>
<li>
<p>用于在训练过程中更新网络权重的算法，如梯度下降、Adam、RMSprop 等。</p>
</li>
<li>
<p><strong>训练（Training）</strong>：</p>
<ul>
<li>使用大量数据和损失函数来调整网络权重和偏置，使模型能够准确地预测或分类新数据。</li>
</ul>
</li>
<li>
<p><strong>过拟合（Overfitting）</strong>：</p>
<ul>
<li>模型在训练数据上表现很好，但在未见过的数据上表现差，即模型对训练数据过度学习。</li>
</ul>
</li>
<li>
<p><strong>正则化（Regularization）</strong>：</p>
<ul>
<li>一种技术，用于减少过拟合，如 L1、L2 正则化或 Dropout。</li>
</ul>
</li>
<li>
<p><strong> 学习率（Learning Rate）</strong>:</p>
<ul>
<li>学习率是一个标量值，用于乘以损失函数关于模型参数的梯度。这个乘积决定了在每次迭代中更新模型参数的步长。</li>
<li>学习率决定了在梯度下降或其他优化算法中，模型参数更新的幅度。如果步长太大，可能会越过最小值点；如果步长太小，训练过程会非常缓慢。</li>
<li>初始学习率的设定通常是基于经验和实验。一些常见的初始学习率值包括 0.01、0.001、0.0001 等。</li>
<li>
<p>在训练过程中，可能需要根据模型的表现调整学习率。一些常见的调整策略包括：</p>
<ul>
<li><strong>固定学习率</strong>：在整个训练过程中保持不变。</li>
<li><strong>学习率衰减</strong>：随着训练的进行逐渐减小学习率。</li>
<li><strong>周期性调整</strong>：周期性地增加或减少学习率。</li>
<li><strong>自适应学习率</strong>：根据模型的损失变化自动调整学习率，如 Adam 优化器。</li>
</ul>
</li>
<li>
<p>如果学习率设置得过小，模型可能需要很长时间才能收敛，或者可能陷入局部最小值而无法达到全局最小值。</p>
</li>
<li>如果学习率设置得过大，模型可能无法收敛，甚至发散。</li>
<li>不同的优化器对学习率的处理方式不同。例如，SGD（随机梯度下降）使用简单的学习率乘以梯度，而 Adam 优化器则使用自适应学习率，为每个参数计算一个单独的学习率。</li>
<li>在训练的初始阶段，逐渐增加学习率，可以帮助模型更快地收敛。</li>
<li>使用学习率查找算法（如 Learning Rate Range Test）可以帮助确定合适的学习率范围。</li>
</ul>
</li>
</ol>
<p>以一个基本的多层感知器（MLP）为例，这是一种常见的前馈神经网络：</p>
<p>当然，下面是一个简化的神经网络示意图，它展示了一个基本的多层感知器（MLP），这是一种常见的前馈神经网络：</p>
<div class="highlight"><pre><span></span><code>    输入层        隐藏层        输出层

    [特征1] --&gt; [神经元1] --&gt; [输出1]
       |           |           |
    [特征2] --&gt; [神经元2] --&gt; [输出2]
       |           |           |
      ...         ...         ...
    [特征N] --&gt; [神经元M] --&gt; [输出K]
</code></pre></div>

<ul>
<li><strong>输入层</strong>：表示神经网络接收的输入特征，每个特征对应一个输入节点。在这个例子中，有N个特征。</li>
<li><strong>隐藏层</strong>：输入层的每个节点都连接到隐藏层的每个神经元。隐藏层可以有多个，每个隐藏层可以包含多个神经元。在这个例子中，隐藏层有M个神经元。</li>
<li><strong>权重和偏置</strong>：每个连接都有一个权重（W），每个神经元都有一个偏置（b）。权重和偏置是神经网络的参数，它们在训练过程中被调整。</li>
<li><strong>激活函数</strong>：隐藏层的每个神经元都有一个激活函数（通常是非线性的），它决定了神经元是否以及如何激活。</li>
<li><strong>输出层</strong>：隐藏层的输出连接到输出层。输出层的每个神经元对应一个最终的输出，在这个例子中，输出层有K个输出。</li>
</ul>
<p>这个示意图展示了数据在神经网络中的流动过程：</p>
<ol>
<li>输入特征被送入输入层。</li>
<li>数据通过权重和偏置加权求和，然后通过激活函数。</li>
<li>激活后的信号在隐藏层中进行处理，可能经过多个隐藏层。</li>
<li>最终，输出层产生模型的预测结果。</li>
</ol>
<h2 id="_2">手写神经网络代码</h2>
<p>作为一个传统程序员, 我以为我是搞不懂这些东西, 直至翻到了这本书 - "Python 神经网络编程"
<img alt="book" src="./images/py_nn_book.png"></p>
<p>这本书对入门者太友好了, 我几乎是一口气读下来的, 很久没有这样读一本技术书籍了, 下面的代码也是我边看书边手敲的, 也就 150 行左右的代码, 就构造了一个简单的神经网络, 用来识别手写数字</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!pip install scipy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scipy.special</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="k">class</span><span class="w"> </span><span class="nc">NeuralNetwork</span><span class="p">:</span>
    <span class="c1"># 初始化神经网络</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># 构建 n 个结点的输入层</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_nodes</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;input_nodes&quot;</span><span class="p">))</span>
        <span class="c1"># 构建 n 个结点的隐藏层</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_nodes&quot;</span><span class="p">))</span>
        <span class="c1"># 构建 n 个结点的输出层</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_nodes</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;output_nodes&quot;</span><span class="p">))</span>
        <span class="c1"># 设置学习率</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">))</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NeuralNetwork: input_nodes: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">input_nodes</span><span class="si">}</span><span class="s2">, hidden_nodes=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span><span class="si">}</span><span class="s2">, output_nodes=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">output_nodes</span><span class="si">}</span><span class="s2">, lr=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># 创建 m*n 的权重矩阵, 数据为 -0.5 到 +0.5 之间的随机值</span>
        <span class="c1">#self.wih = numpy.random.rand(self.hidden_nodes, self.input_nodes) - 0.5</span>
        <span class="c1">#self.who = numpy.random.rand(self.output_nodes, self.hidden_nodes) - 0.5</span>

        <span class="c1"># 创建 m*n 的权重矩阵, 数据为正态分布的的权重矩阵, 以 0.0 为分布中心值, 以 3^(-0.5) 为标准方差</span>
        <span class="c1"># weight between input layer and hidden layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wih</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">pow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_nodes</span><span class="p">))</span>
        <span class="c1"># weight between hidden layer and output layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">who</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">pow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span><span class="p">))</span>

        <span class="c1"># 设置激活函数</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">expit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># 训练神经网络</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_list</span><span class="p">,</span> <span class="n">target_list</span><span class="p">):</span>
        <span class="c1"># convert inputs list to 2d array</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">ndmin</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">target_list</span><span class="p">,</span> <span class="n">ndmin</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

        <span class="c1"># calculate signals into hidden layer</span>
        <span class="n">hidden_inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wih</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="c1"># calculate the signals emerging from hidden layer</span>
        <span class="n">hidden_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">hidden_inputs</span><span class="p">)</span>

        <span class="n">final_inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">who</span><span class="p">,</span> <span class="n">hidden_outputs</span><span class="p">)</span>
        <span class="c1"># calculate signals emerging from final output layer</span>
        <span class="n">final_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">final_inputs</span><span class="p">)</span>

        <span class="c1"># 计算误差 target - actual</span>
        <span class="n">output_errors</span> <span class="o">=</span> <span class="n">targets</span> <span class="o">-</span> <span class="n">final_outputs</span>
        <span class="c1"># 隐藏层的误差 hidder_errors 可由权重 output_errors 经权重 who 进行分割得出 </span>
        <span class="n">hidden_errors</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">who</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">output_errors</span><span class="p">)</span>

        <span class="c1"># 更新隐藏层与输出层之间的权重</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">who</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">((</span><span class="n">output_errors</span> <span class="o">*</span> <span class="n">final_outputs</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">final_outputs</span><span class="p">)),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">hidden_outputs</span><span class="p">))</span>
        <span class="c1"># 更新输入层与隐藏层之间的权重</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wih</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">((</span><span class="n">hidden_errors</span> <span class="o">*</span> <span class="n">hidden_outputs</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">hidden_outputs</span><span class="p">)),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>

    <span class="c1"># 查询神经网络, 接收输入, 产生输出</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">query</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_list</span><span class="p">):</span>
        <span class="c1"># convert inputs list to 2d array</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">ndmin</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="c1"># calculate signals into hidden layer</span>
        <span class="n">hidden_inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wih</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="c1"># calculate the signals emerging from hidden layer</span>
        <span class="n">hidden_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">hidden_inputs</span><span class="p">)</span>
        <span class="n">final_inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">who</span><span class="p">,</span> <span class="n">hidden_outputs</span><span class="p">)</span>
        <span class="c1"># calculate signals emerging from final output layer</span>
        <span class="n">final_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">final_inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">final_outputs</span>


<span class="k">def</span><span class="w"> </span><span class="nf">load_data</span><span class="p">(</span><span class="n">data_file</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">data_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">line_num</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_file</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">data_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
            <span class="n">line_num</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">line_num</span> <span class="o">&gt;=</span> <span class="n">count</span><span class="p">:</span>
                <span class="k">break</span>
    <span class="k">return</span> <span class="n">data_list</span>

<span class="c1"># 训练数据</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">training_data_list</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">):</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>

    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># go through all records in the training data set</span>
        <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">training_data_list</span><span class="p">:</span>
            <span class="c1"># split the record by the &#39;,&#39; commas</span>
            <span class="n">all_values</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
            <span class="c1"># scale and shift the inputs</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">asfarray</span><span class="p">(</span><span class="n">all_values</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">/</span> <span class="mf">255.0</span> <span class="o">*</span> <span class="mf">0.99</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.01</span>
            <span class="c1"># create the target output values (all 0.01, except the desired label which is 0.99)</span>
            <span class="n">targets</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_nodes</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.01</span>
            <span class="c1"># all_values[0] is the target label for this record</span>
            <span class="n">targets</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">all_values</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="o">=</span> <span class="mf">0.99</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

<span class="c1"># 测试数据</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">test_data_list</span><span class="p">):</span>
    <span class="n">scorecard</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">loop_num</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># go through all the records in the test data set</span>
    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">test_data_list</span><span class="p">:</span>
        <span class="c1"># split the record by the &#39;,&#39; commas</span>
        <span class="n">all_values</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
        <span class="c1"># correct answer is first value</span>
        <span class="n">correct_label</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">all_values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="c1"># scale and shift the inputs</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">asfarray</span><span class="p">(</span><span class="n">all_values</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">/</span> <span class="mf">255.0</span> <span class="o">*</span> <span class="mf">0.99</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.01</span>
        <span class="c1"># query the network</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="c1"># the index of the highest value corresponds to the label</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loop_num</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">draw_digit</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;infer to </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2"> from </span><span class="si">{</span><span class="n">outputs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># append correct or incorrect to list</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">label</span> <span class="o">==</span> <span class="n">correct_label</span><span class="p">):</span>
            <span class="c1"># network&#39;s answer matches correct answer, add 1 to scorecard</span>
            <span class="n">scorecard</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># network&#39;s answer doesn&#39;t match correct answer, add 0 to scorecard</span>
            <span class="n">scorecard</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">loop_num</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">scorecard</span>

<span class="c1"># 维护手写数字</span>
<span class="k">def</span><span class="w"> </span><span class="nf">draw_digit</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;data format: digit 28*28=784 points, data: </span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
    <span class="n">image_array</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">asfarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_array</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greys&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># 构造神经网络</span>
    <span class="n">nn</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">(</span><span class="n">input_nodes</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_nodes</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">output_nodes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="c1"># 取 1000 条数据来训练</span>
    <span class="n">data_count</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="c1"># load the mnist training data CSV file into a list - 60000 titakt</span>
    <span class="n">training_data_list</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;mnist_train.csv&quot;</span><span class="p">,</span> <span class="n">data_count</span><span class="p">)</span>

    <span class="n">train</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">training_data_list</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="c1"># load the mnist test data CSV file into a list - 10000 totally</span>
    <span class="n">test_data_list</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;mnist_test.csv&quot;</span><span class="p">,</span> <span class="n">data_count</span><span class="p">)</span>

    <span class="n">scorecard</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">test_data_list</span><span class="p">)</span>
    <span class="c1"># calculate the performance score, the fraction of correct answers</span>
    <span class="n">scorecard_array</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">scorecard</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;performance = &quot;</span><span class="p">,</span> <span class="n">scorecard_array</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">scorecard_array</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</code></pre></div>

<p>输出如下</p>
<p><img alt="output" src="./images/py_nn_output.png"></p>
<p>我们构造了一个三层的神经网络, 输入层有 784 个节点, 隐藏层有 200 个节点, 输出层有 10 个节点
以上面的 "7" 这个数字为例, 可以看到输出的节点中第 7 个节点的值最高, 也就是 7 的可能性最大, 从而识别出这个手写数字</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="./tag/journal.html">journal</a>
      <a href="./tag/blog.html">blog</a>
    </p>
  </div>


  <div class="neighbors">
    <a class="btn float-left" href="./deepstream-ru-men-yi.html" title="DeepStream 入门一">
      <i class="fa fa-angle-left"></i> Previous Post
    </a>
    <a class="btn float-right" href="./jiu-kai-fa-ban-zhi-fei-wu-li-yong.html" title="旧开发板之废物利用">
      Next Post <i class="fa fa-angle-right"></i>
    </a>
  </div>

  <div class="related-posts">
    <h4>You might enjoy</h4>
    <ul class="related-posts">
      <li><a href="./operator-terraform-dui-chuan-tong-yun-wei-de-gai-bian.html">Operator + Terraform 对传统运维的改变</a></li>
      <li><a href="./shi-yong-di-yi-xing-yuan-li-zuo-jia-gou-she-ji.html">使用第一性原理做架构设计</a></li>
      <li><a href="./zhi-chang-zhong-na-xie-huo-de-zui-jiu-de-fang-fa-lun-suo-xie.html">职场中那些“活得最久”的方法论缩写</a></li>
      <li><a href="./zui-tong-yong-de-6-da-yan-jiang-kuang-jia.html">最通用的 6 大演讲框架</a></li>
      <li><a href="./bie-liao-2025.html">别了, 2025</a></li>
    </ul>
  </div>



<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'wfblog';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
    Please enable JavaScript to view comments.
</noscript>
<!-- End Disqus -->
</article>

<footer>
<p>&copy; 2010 ~ 2030  Walter Fan <a href="https://beian.miit.gov.cn" target="_blank">皖ICP备20001876号-1</a></p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p><!-- StatusCake -->

<!-- End StatusCake --></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Walter Fan's Blog ",
  "url" : ".",
  "image": "./images/walterfan.jpg",
  "description": "an old programmer never die, he just branch to a new address."
}
</script>
</body>
</html>