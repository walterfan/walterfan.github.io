
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="./theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="./theme/pygments/github.min.css">



  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/solid.css">


  <link rel="shortcut icon" href="./images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="./images/favicon.ico" type="image/x-icon">










 

<meta name="author" content="Walter Fan" />
<meta name="description" content="Daily minute" />
<meta name="keywords" content="webrtc">


  <meta property="og:site_name" content="Walter Fan's Blog"/>
  <meta property="og:title" content="WebRTC 的音视频如何同步"/>
  <meta property="og:description" content="Daily minute"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="./webrtc-de-yin-shi-pin-ru-he-tong-bu.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2023-07-19 10:20:00+08:00"/>
  <meta property="article:modified_time" content="2023-07-19 19:30:00+08:00"/>
  <meta property="article:author" content="./author/walter-fan.html">
  <meta property="article:section" content="Journal"/>
  <meta property="article:tag" content="webrtc"/>
  <meta property="og:image" content="./images/walterfan.jpg">

  <title>Walter Fan's Blog &ndash; WebRTC 的音视频如何同步</title>


</head>
<body class="light-theme">

<aside>
  <div>
    <a href="./">
      <img src="./images/walterfan.jpg" alt="Walter Fan" title="Walter Fan">
    </a>

    <h1>
      <a href="./">Walter Fan</a>
    </h1>

    <p>手握灵珠常奋笔, 心开天籁不吹箫</p>


    <nav>
      <ul class="list">



          <li>
            <a target="_self" href="/pims/tao" >tao</a>
          </li>
          <li>
            <a target="_self" href="interest.html" >interest</a>
          </li>
          <li>
            <a target="_self" href="/wordpress" >notebook</a>
          </li>
          <li>
            <a target="_self" href="bookmark.html" >bookmark</a>
          </li>
          <li>
            <a target="_self" href="/webrtc/examples/index.html" >webrtc</a>
          </li>
          <li>
            <a target="_self" href="https://github.com/walterfan" >github</a>
          </li>
          <li>
            <a target="_self" href="https://www.jianshu.com/u/e0b365801f48" >技术文章</a>
          </li>
      </ul>
    </nav>

    <ul class="social">
      <li>
        <a class="sc-github"
           href="http://github.com/walterfan"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
      <li>
        <a class="sc-weibo"
           href="http://weibo.com/walterfan"
           target="_blank">
          <i class="fa-brands fa-weibo"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="webrtc-de-yin-shi-pin-ru-he-tong-bu">WebRTC 的音视频如何同步</h1>
    <p>
      Posted on Wed 19 July 2023 in <a href="./category/journal.html">Journal</a>

    </p>
  </header>


  <div>
    <table>
<thead>
<tr>
<th><strong>Abstract</strong></th>
<th>WebRTC 的音视频如何同步s</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Authors</strong></td>
<td><a href="https://www.fanyamin.com" referrerpolicy="no-referrer" rel="noopener noreferrer" target="_blank" title="Walter Fan">Walter Fan</a></td>
</tr>
<tr>
<td> <strong>Category</strong>  </td>
<td> learning note  </td>
</tr>
<tr>
<td><strong>Status</strong></td>
<td>v1.0</td>
</tr>
<tr>
<td><strong>Updated</strong></td>
<td>2023-07-19</td>
</tr>
<tr>
<td><strong>License</strong></td>
<td><a href="http://creativecommons.org/licenses/by-nc-nd/4.0" referrerpolicy="no-referrer" rel="noopener noreferrer" target="_blank" title="CC-BY-NC-ND 4.0">CC-BY-NC-ND 4.0</a></td>
</tr>
</tbody>
</table>
<p>在网络视频会议中， 我们常会遇到音视频不同步的问题， 我们有一个专有名词 lip-sync 唇同步来描述这类问题，当我们看到人的嘴唇动作与听到的声音对不上的时候，不同步的问题就出现了</p>
<p>而在线会议中， 听见清晰的声音是优先级最高的， 人耳对于声音的延迟是很敏感的</p>
<p>根据 T-REC-G.114-200305 中的描述</p>
<ul>
<li>大于~280ms 有些用户就会不满意</li>
<li>大于~380ms 多数用户就会不满意</li>
<li>大于~500ms 几乎所有用户就会不满意</li>
</ul>
<p>我们就尽量使得声音的延迟在 280 ms 之内，这是解决 lip-sync 问题的前提, 声音不好的严重程序超过音视频不同步。</p>
<p>我们可以定义一个 sync_diff 值 来表示音频帧和视频帧之间的时间差
* 正值表示音频领先于视频
* 负值表示音频落后于视频</p>
<p>ITU 对此给出以下的阈值:
* 不可感知 Undetectability (-100ms, +25ms)
* 可感知 Detectability: (-125ms, +45ms)
* 可接受 Acceptability: (–185ms, +90 ms)
* 影响用户 Impact user experience (-∞, -185ms) ∪ (+90ms,∞)</p>
<p>(ITU-R BT.1359-1, Relative Timing of Sound and Vision for Broadcasting" 1998. Retrieved 30 May 2015)</p>
<p>当我们在播放一个视频帧及对应的音频帧的时候，要计算一下这个 sync_diff</p>
<div class="highlight"><pre><span></span><code>sync_diff = audio_frame_time - video_frame_time
</code></pre></div>

<p>如果这个 sync_diff 大于 90ms, 也就是音频包到得过早，就会有音视频不同步的问题 - 声音听到了，嘴巴没跟上.</p>
<p>如果这个 sync_diff 小于 -185ms, 也就是视频包到得过早，就会有音视频不同步的问题 - 嘴巴在动，声音没跟上.</p>
<h1 id="_1">不同步的原因</h1>
<p><img alt="lip sync 1" src="https://upload-images.jianshu.io/upload_images/1598924-68adcc504fb552ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<p>这个问题的原因主要在于音频的采集， 编码，传输， 解码， 播放与视频的采集，编码，传输，解码以及渲染一般是分开进行的，因为音频和视频采集自不同的设备，即它们的来源不同，在网络上传输也会有延迟，也由不同的设备进行播放，这样如果在接收方不采取措施进行时间同步，就会极有可能看到口型和听到的声音对不上的情况。</p>
<p>由此派生出 ３ 个小问题：</p>
<ol>
<li>如何将来自同一个人或设备的多路 audio 及 video stream关联起来?</li>
<li>如何将 RTP 中的时间戳 timestamp 映射到发送方的音视频采集时间</li>
<li>如何调整音频或者视频帧的播放时间，让它们怎么之间相对同步？</li>
</ol>
<h1 id="_2">解决方案</h1>
<h2 id="1">1. 如何将来自同一个人或设备的音视频流关联起来?</h2>
<p>对于多媒体会话，每种类型的媒体（例如音频或视频）一般会在单独的 RTP 会话中发送，发送方会在 RTCP SDES 消息中指明
接收方通过 CNAME 项关联要同步的RTP流, 而这个 CNAME 包含在发送方所发送的 RTCP SDES 中</p>
<p>SDES 数据包包含常规包头，有效负载类型为 202，项目计数等于数据包中 SSRC/CSRC 块的数量，后跟零个或多个 SSRC/CSRC 块，其中包含有关特定 SSRC 或 CSRC，每个都与 32 位边界对齐。</p>
<div class="highlight"><pre><span></span><code>    0               1               2               3
    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |V=2|P|    SC   |  PT=SDES=202  |            length L           |
    +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+
    |                          SSRC/CSRC_1                          |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                           SDES items                          |
    |                              ...                              |
    +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+
    |                          SSRC/CSRC_2                          |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                           SDES items                          |
    |                              ...                              |
    +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+
</code></pre></div>

<p>CNAME 项在每个 SDES 数据包中都是必需的，而 SDES 数据包又是每个复合 RTCP 数据包中的必需部分。</p>
<p>与 SSRC 标识符一样，CNAME 必须与其他会话参与者的 CNAME 不同。 但 CNAME 不应随机选择 CNAME 标识符，而应允许个人或程序通过 CNAME 内容来定位其来源。</p>
<div class="highlight"><pre><span></span><code>    0               1               2               3
    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |    CNAME=1    |     length    | user and domain name         ...
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
</code></pre></div>

<p>例如 Alice 向外发送一路音频流，一路视频流, 这两路流会使用不同的 SSRC, 但是在其所发送的 RTCP SDES 消息会使用相同的 CNAME.</p>
<ul>
<li>RTP SSRC 1 ~ CNAME 1</li>
<li>RTP SSRC 2 ~ CNAME 1</li>
</ul>
<h2 id="2">2. 同步的时间如何计算</h2>
<p>来自同一个终端用户的音频和视频, 在编码发送的 RTP 包中有一个 timestamp, 这个时间戳表示媒体流的捕捉时间。
同时, 作为发送者也会发送 RTCP Sender Report, 其中包含发送的 RTP timestamp 和 NTP timestamp 的映射关系，这样我们在接收方就可以把 RTP 包里的 </p>
<p><img alt="lip sync flow" src="https://upload-images.jianshu.io/upload_images/1598924-b9512c88e2c0f871.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<p>对于每个 RTP 流，发送方定期发出 RTCP SR, 其中包含一对时间戳：</p>
<p>NTP 时间戳以及与该 RTP 流关联的相应 RTP 时间戳。</p>
<p>这对时间戳传达每个媒体流的 NTP 时间和 RTP 时间之间的关系。</p>
<p>先回顾一下 RTP packet 和 RTCP sender report</p>
<ul>
<li>RTP 包结构</li>
</ul>
<div class="highlight"><pre><span></span><code>    0                   1                   2                   3
    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |V=2|P|X|  CC   |M|     PT      |       sequence number         |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |                           timestamp                           |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |           synchronization source (SSRC) identifier            |
   +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+
   |            contributing source (CSRC) identifiers             |
   |                             ....                              |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
</code></pre></div>

<ul>
<li>RTCP Sender Report 结构</li>
</ul>
<div class="highlight"><pre><span></span><code>         0                   1                   2                   3
         0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   header |V=2|P|    RC   |   PT=SR=200   |             length            |
         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
         |                         SSRC of sender                        |
         +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+
   sender |              NTP timestamp, most significant word             |
   info   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
         |             NTP timestamp, least significant word             |
         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
         |                         RTP timestamp                         |
         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
         |                     sender&#39;s packet count                     |
         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
         |                      sender&#39;s octet count                     |
         +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+
   report |                 SSRC_1 (SSRC of first source)                 |
   block  +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   1    | fraction lost |       cumulative number of packets lost       |
         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
         |           extended highest sequence number received           |
         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
         |                      interarrival jitter                      |
         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
         |                         last SR (LSR)                         |
         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
         |                   delay since last SR (DLSR)                  |
         +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+
   report |                 SSRC_2 (SSRC of second source)                |
   block  +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   2    :                               ...                             :
         +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+
         |                  profile-specific extensions                  |
         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
</code></pre></div>

<p>通过 NTP timestamp 和 RTP timestamp 之间的映射, 我们可以知道 audio 包的时间和 video 包的时间。</p>
<p>具体的计算可以参见 WebRTC 的 RtpToNtpEstimator 类, 它将收到的若干 SR 中的 NTP time 和 RTP timestamp 保存下来，然后  应用最小二乘法来估算后续 RTP timestamp 所对应的 NTP timestamp, 大致为用最近 N=20 个 RTCP SR 包的 ntp timestamp 和 rtp timestamp 的构造出线性关系 y = ax + b， 通过最小二乘法来计算收到的 RTP 包对应的 ntp timestamp.</p>
<div class="highlight"><pre><span></span><code>// Converts an RTP timestamp to the NTP domain.
// The class needs to be trained with (at least 2) RTP/NTP timestamp pairs from
// RTCP sender reports before the convertion can be done.
class RtpToNtpEstimator {
      public:
            //...

            enum UpdateResult { kInvalidMeasurement, kSameMeasurement, kNewMeasurement };
            // Updates measurements with RTP/NTP timestamp pair from a RTCP sender report.
            UpdateResult UpdateMeasurements(NtpTime ntp, uint32_t rtp_timestamp);

            // Converts an RTP timestamp to the NTP domain.
            // Returns invalid NtpTime (i.e. NtpTime(0)) on failure.
            NtpTime Estimate(uint32_t rtp_timestamp) const;

            // Returns estimated rtp_timestamp frequency, or 0 on failure.
            double EstimatedFrequencyKhz() const;

      private:
            // Estimated parameters from RTP and NTP timestamp pairs in `measurements_`.
            // Defines linear estimation: NtpTime (in units of 1s/2^32) =
            //   `Parameters::slope` * rtp_timestamp + `Parameters::offset`.
            struct Parameters {
                  double slope;
                  double offset;
            };

            // RTP and NTP timestamp pair from a RTCP SR report.
            struct RtcpMeasurement {
                  NtpTime ntp_time;
                  int64_t unwrapped_rtp_timestamp;
            };

            void UpdateParameters();

            int consecutive_invalid_samples_ = 0;
            std::list&lt;RtcpMeasurement&gt; measurements_;
            absl::optional&lt;Parameters&gt; params_;
            mutable RtpTimestampUnwrapper unwrapper_;
};
</code></pre></div>

<h2 id="3">3. 调整播放和渲染时间</h2>
<p>一般我们会以 audio 为主,  video 向 audio 靠拢, 两者时间一致也就会达到 lip sync 音视频同步</p>
<ol>
<li>audio 包先来, video 包后来: audio 包放在 jitter buffer 时等一会儿, 但是这个时间是有限的, 音频的流畅是首先要保证的, 视频跟不上可以降低视频的码率</li>
<li>video 包先来, audio 包后来: video 包要等 audio 包来, 这是为了让音视频同步要付出的代价</li>
</ol>
<p>一般以音频为主流 master stream，视频为从流 slave stream。 一般方法是接收方维护音频流的缓冲区的管理，并通过将视频 RTP 时间戳转换为正确从属于音频流的时间戳来调整视频流的播放。</p>
<p>当带有RTP时间戳 RTPv的视频帧到达接收器时，接收器通过四个步骤将RTP时间戳 RTPv 映射到视频设备时间戳VTB( Video Time Base)，如图所示。</p>
<ol>
<li>
<p>使用 Video RTCP SR 中的 RTP/NTP 时间戳对建立的映射，将视频 RTP 时间戳  RTPv 映射到发送方 NTP 时间。</p>
</li>
<li>
<p>根据该 NTP 时间戳，使用 Audio RTCP SR 中的 RTP/NTP 时间戳对建立的映射，计算来自发送方的相应音频 RTPa 时间戳。
此时，视频RTP时间戳被映射到音频RTP 包的相同时间基准。</p>
</li>
<li>
<p>根据该音频 RTP 时间戳，使用卡尔曼滤波的方法计算音频设备时间基准中的相应时间戳。 结果是音频设备时间基准 ATB(Audio Time Base) 中的时间戳。</p>
</li>
<li>
<p>根据 ATB，使用偏移量 AtoV 计算视频设备时基 VTB 中的相应时间戳。</p>
</li>
</ol>
<p>接收方需要确保带有 RTP 时间戳 RTPv 的视频帧使用所计算出的发送方视频设备时间基准 VTB 播放。</p>
<div class="highlight"><pre><span></span><code>      AtoV = V_time - A_Time/(audio sample rate)
</code></pre></div>

<p>注: 
* AtoV: 音频相较视频的偏移量
* ATB: Audio device Time Base 音频设备的时间基准
* VTB: Video device Time Base 视频设备的时间基准</p>
<p>具体方法可以参见 https://www.ccexpert.us/video-conferencing/using-rtcp-for-media-synchronization.html)
<img alt="av sync" src="https://upload-images.jianshu.io/upload_images/1598924-91a3414d50409d4a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<p>WebRTC 的做法原理上差不多，实现略有不同，可以参见 WebRTC 的源代码 StreamSynchronization 类和 RtpStreamsSynchronizer 类</p>
<p>大致上它会计算出 video 的延迟 </p>
<div class="highlight"><pre><span></span><code>current_delay_ms = max(min_playout_delay_ms, jitter_delay_ms + decode_time _ms + render_delay_ms)
</code></pre></div>

<p>然后再计算视频相对于音频的延迟 <code>relative_delay_ms</code>, 
- 如果它大于0， 视频比音频慢，减小视频延迟（主要是调整 jitter buffer delay)，或者是增大音频延迟， 取决于阈值 base_target_delay_ms
- 如果它小于0， 音频比视频慢，减小音频延迟，或者是增大视频延迟， 取决于阈值base_target_delay_ms</p>
<p>base_target_delay_ms 的比较逻辑参见<a href="https://source.chromium.org/chromium/chromium/src/+/main:third_party/webrtc/video/stream_synchronization.cc;l=64" referrerpolicy="no-referrer" rel="noopener noreferrer" target="_blank" title="StreamSynchronization::ComputeDelays">StreamSynchronization::ComputeDelays</a>, </p>
<div class="highlight"><pre><span></span><code>if (diff_ms &gt; 0) {
      // The minimum video delay is longer than the current audio delay.
      // We need to decrease extra video delay, or add extra audio delay.
      if (video_delay_.extra_ms &gt; base_target_delay_ms_) {
            // We have extra delay added to ViE. Reduce this delay before adding
            // extra delay to VoE.
            video_delay_.extra_ms -= diff_ms;
            audio_delay_.extra_ms = base_target_delay_ms_;
      } else {  // video_delay_.extra_ms &gt; 0
            // We have no extra video delay to remove, increase the audio delay.
            audio_delay_.extra_ms += diff_ms;
            video_delay_.extra_ms = base_target_delay_ms_;
      }
      } else {  // if (diff_ms &gt; 0)
      // The video delay is lower than the current audio delay.
      // We need to decrease extra audio delay, or add extra video delay.
      if (audio_delay_.extra_ms &gt; base_target_delay_ms_) {
            // We have extra delay in VoiceEngine.
            // Start with decreasing the voice delay.
            // Note: diff_ms is negative; add the negative difference.
            audio_delay_.extra_ms += diff_ms;
            video_delay_.extra_ms = base_target_delay_ms_;
      } else {  // audio_delay_.extra_ms &gt; base_target_delay_ms_
            // We have no extra delay in VoiceEngine, increase the video delay.
            // Note: diff_ms is negative; subtract the negative difference.
            video_delay_.extra_ms -= diff_ms;  // X - (-Y) = X + Y.
            audio_delay_.extra_ms = base_target_delay_ms_;
      }
}
</code></pre></div>

<p>更多细节在 WebRTC 的代码中
* class StreamSynchronization 
* class RtpStreamsSynchronizer</p>
<p>通过StreamSynchronization::ComputeDelays计算出音频和视频的相对延迟，如果相对延迟很小( &lt; 30ms), 则无需调整音视频的播放时间，如果相对延迟很大， 则以 80ms 的幅度进行逐步调整。 与传统的只调视频延迟，不调音频延迟， WebRTC 会两边都调点，使得音视频的时间彼此靠近，前提是音频的延迟是在上面提到的可接受范围之内。</p>
<h1 id="_3">参考资料</h1>
<ul>
<li>https://www.ciscopress.com/articles/article.asp?p=705533&amp;seqNum=6</li>
<li>https://www.ccexpert.us/video-conferencing/using-rtcp-for-media-synchronization.html</li>
<li>https://testrtc.com/docs/how-do-you-find-lip-sync-issues-in-webrtc/</li>
<li>https://en.wikipedia.org/wiki/Audio-to-video_synchronization</li>
<li>https://www.simplehelp.net/2018/05/29/how-to-fix-out-of-sync-audio-video-in-an-mkv-mp4-or-avi/
*<a href="https://datatracker.ietf.org/doc/html/rfc6051" referrerpolicy="no-referrer" rel="noopener noreferrer" target="_blank" title="RFC6051">RFC6051</a>: Rapid Synchronisation of RTP Flows</li>
</ul>
<hr/>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-nd/4.0/" referrerpolicy="no-referrer" rel="noopener noreferrer" target="_blank" title="知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</a>进行许可。</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="./tag/webrtc.html">webrtc</a>
    </p>
  </div>


  <div class="neighbors">
    <a class="btn float-left" href="./software-20.html" title="Software 2.0">
      <i class="fa fa-angle-left"></i> Previous Post
    </a>
    <a class="btn float-right" href="./yi-ni-zi-ji-de-sheng-huo-fang-shi-du-guo-ni-de-yi-sheng.html" title="以你自己的生活方式度过你的一生">
      Next Post <i class="fa fa-angle-right"></i>
    </a>
  </div>

  <div class="related-posts">
    <h4>You might enjoy</h4>
    <ul class="related-posts">
      <li><a href="./dtls-wo-shou-wei-shi-yao-chang-shi-bai.html">DTLS 握手为什么常失败</a></li>
      <li><a href="./delay-based-controller.html">基于延迟的带宽评估</a></li>
      <li><a href="./yong-sai-kong-zhi-ji-zhu-de-bi-ji-san-twcc-zai-libwebrtc-zhong-de-shi-xian.html">拥塞控制技术的笔记三: TWCC 在 libwebrtc 中的实现</a></li>
      <li><a href="./yong-sai-kong-zhi-ji-zhu-de-bi-ji-er-twcc.html">拥塞控制技术的笔记二: TWCC</a></li>
      <li><a href="./yong-sai-kong-zhi-ji-zhu-de-bi-ji-yi-li-lun-pian.html">拥塞控制技术的笔记一: 理论篇</a></li>
    </ul>
  </div>



<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'wfblog';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
    Please enable JavaScript to view comments.
</noscript>
<!-- End Disqus -->
</article>

<footer>
<p>&copy; 2010 ~ 2030  Walter Fan <a href="https://beian.miit.gov.cn" target="_blank">皖ICP备20001876号-1</a></p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p><!-- StatusCake -->

<!-- End StatusCake --></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Walter Fan's Blog ",
  "url" : ".",
  "image": "./images/walterfan.jpg",
  "description": "an old programmer never die, he just branch to a new address."
}
</script>
</body>
</html>